version: 2

models: 
  - name: int_parse_scheduler_logs
    description: >
      This model takes the raw log sink table of all the "DAGrun Finished" logs from
      Composer and parses out a bunch of relevent fields about the DAG run from the
      text payload.
    columns:
      - name: insert_id
        tests: 
          - not_null
          - unique
      - name: log_timestamp
      - name: received_timestamp
      - name: project_id
        tests: 
          - not_null
      - name: environment_name
      - name: dag_id
        tests: 
          - not_null
      - name: execution_date
      - name: run_id
        tests:
          - not_null
      - name: run_start_date
      - name: run_end_date
      - name: run_duration
      - name: state
        tests:
          - accepted_values:
              values: ['success', 'failed']
      - name: external_trigger
      - name: run_type
      - name: data_interval_start
      - name: data_interval_end
      - name: dag_hash
  
  - name: int_group_scheduler_logs_by_dag
    description: >
      This model takes our parsed scheduler logs table and groups by dag run (dag_id and
      run_id). When tasks within a given dag run get re-run (e.g. a task fails and is
      re-run manually from the frontend), the scheduler logs a new entry for the dag run
      when it finishes subsequent times.
      
      To handle this, we group all those entries together, and aggregate the fields
      appropriately. We take the min() of start times, the max() of end times, the sum()
      of durations, and a handful of other fields, including `state` from the most
      recent log entry.
    columns:
      - name: dag_id
        tests: 
          - not_null
      - name: run_id
        tests:
          - not_null
      - name: insert_id
      - name: log_timestamp
      - name: received_timestamp
      - name: project_id
        tests: 
          - not_null
      - name: environment_name
      - name: execution_date
      - name: run_start_date
      - name: run_end_date
      - name: run_duration
      - name: state
        tests:
          - accepted_values:
              values: ['success', 'failed']
      - name: external_trigger
      - name: run_type
      - name: data_interval_start
        description: >
          The "data_interval_" fields come directly from Airflow, and theoretically
          represent the range of data that the DAG run is processing. In practice, we
          are not actually using these fields in our DAGs to do any filtering.

          More details: https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dag-run.html#data-interval
      - name: data_interval_end
      - name: dag_hash
    tests:
      - dbt_utils.unique_combination_of_columns:
          combination_of_columns: [dag_id, run_id]